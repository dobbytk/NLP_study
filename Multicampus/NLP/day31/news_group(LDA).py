# -*- coding: utf-8 -*-
"""news_group20(LDA).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1j8yHHgLQo4VRTZXzryEEDhiBMJaA7_T7
"""

# Commented out IPython magic to ensure Python compatibility.
# Latent Dirichlet Allocation (LDA)
# ---------------------------------
import numpy as np
import pickle
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation as LDA

# %cd '/content/drive/MyDrive/Colab Notebooks'

# 전처리가 완료된 한글 코퍼스를 읽어온다.
with open('./newsgroup20.pkl', 'rb') as f:
    subject, text, target = pickle.load(f)

n_target = len(set(target))

# TF-IDF matrix를 생성한다.
tf_vector = TfidfVectorizer(max_features = 500)
tfidf = tf_vector.fit_transform(text)
print(tfidf.shape)

vocab = tf_vector.get_feature_names()
print(vocab[:20])

# Latent Dirichlet Allocation (LDA)
# ---------------------------------
# Return 값이 Document-Topic distribution이다.
# iteration 횟수가 max_iter까지 가면 아직 수렴하지 않은 것이다.
# 아직 수렴하지 않은 경우 mat_iter를 증가시켜야 한다.
model = LDA(n_components = n_target, 
            learning_method='online', 
            evaluate_every=5, 
            max_iter=1000, 
            verbose=1)

doc_topic = model.fit_transform(tfidf)

# 문서 별 Topic 번호를 확인한다. (문서 10개만 확인)
for i in range(10):
    print('문서-{:d} : topic = {:02d}, target = {:02d}'.format(i, np.argmax(doc_topic[i:(i+1), :][0]), target[i]))

text[0], text[7]

# topic_term 행렬에서 topic 별로 중요 단어를 표시한다
topic_term = model.components_
for i in range(len(topic_term)):
    idx = np.flipud(topic_term[i].argsort())[:10]
    print('토픽-{:2d} : '.format(i+1), end='')
    for n in idx:
        print('{:s} '.format(vocab[n]), end='')
    print()

